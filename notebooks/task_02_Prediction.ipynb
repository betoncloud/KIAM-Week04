{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rossmann Pharmaceuticals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script is to conduct prediction of 2 weeks sales across stores of Rossmann Pharmaceuticals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "1. Task 2.1 - Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 09:39:02,727 - INFO - Loading data from csv file\n",
      "2025-01-14 09:39:03,553 - INFO - Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from scripts.load_data import load_data\n",
    "logging.info(\"Loading data from csv file\")\n",
    "df_store= load_data('../data/store.csv')\n",
    "df_test=load_data('../data/test.csv')\n",
    "df_train=load_data('../data/train.csv')\n",
    "logging.info(\"Data loaded successfully\")\n",
    "\n",
    "# Merging train data with store data to analyze store-level details\n",
    "merged_data = pd.merge(df_train, df_store, on='Store')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1017209 entries, 0 to 1017208\n",
      "Data columns (total 18 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   Store                      1017209 non-null  int64  \n",
      " 1   DayOfWeek                  1017209 non-null  int64  \n",
      " 2   Date                       1017209 non-null  object \n",
      " 3   Sales                      1017209 non-null  int64  \n",
      " 4   Customers                  1017209 non-null  int64  \n",
      " 5   Open                       1017209 non-null  int64  \n",
      " 6   Promo                      1017209 non-null  int64  \n",
      " 7   StateHoliday               1017209 non-null  object \n",
      " 8   SchoolHoliday              1017209 non-null  int64  \n",
      " 9   StoreType                  1017209 non-null  object \n",
      " 10  Assortment                 1017209 non-null  object \n",
      " 11  CompetitionDistance        1014567 non-null  float64\n",
      " 12  CompetitionOpenSinceMonth  693861 non-null   float64\n",
      " 13  CompetitionOpenSinceYear   693861 non-null   float64\n",
      " 14  Promo2                     1017209 non-null  int64  \n",
      " 15  Promo2SinceWeek            509178 non-null   float64\n",
      " 16  Promo2SinceYear            509178 non-null   float64\n",
      " 17  PromoInterval              509178 non-null   object \n",
      "dtypes: float64(5), int64(8), object(5)\n",
      "memory usage: 139.7+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                             0\n",
       "DayOfWeek                         0\n",
       "Date                              0\n",
       "Sales                             0\n",
       "Customers                         0\n",
       "Open                              0\n",
       "Promo                             0\n",
       "StateHoliday                      0\n",
       "SchoolHoliday                     0\n",
       "StoreType                         0\n",
       "Assortment                        0\n",
       "CompetitionDistance            2642\n",
       "CompetitionOpenSinceMonth    323348\n",
       "CompetitionOpenSinceYear     323348\n",
       "Promo2                            0\n",
       "Promo2SinceWeek              508031\n",
       "Promo2SinceYear              508031\n",
       "PromoInterval                508031\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "merged_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Date column to datetime format for test and train data\n",
    "df_test['Date'] = pd.to_datetime(df_test['Date'])\n",
    "df_train['Date'] = pd.to_datetime(df_train['Date'])\n",
    "\n",
    "# Extract holidays from train dataset\n",
    "state_holidays = df_train[df_train['StateHoliday'] != '0']['Date'].unique()\n",
    "state_holidays = np.array(sorted(state_holidays))  # Convert to a sorted numpy array for efficiency\n",
    "\n",
    "# Helper functions\n",
    "def get_weekday_or_weekend(date):\n",
    "    return \"Weekend\" if date.weekday() >= 5 else \"Weekday\"\n",
    "\n",
    "def days_to_next_holiday(date, holidays):\n",
    "    future_holidays = holidays[holidays > date]\n",
    "    return (future_holidays[0] - date).days if future_holidays.size > 0 else None\n",
    "\n",
    "def days_after_last_holiday(date, holidays):\n",
    "    past_holidays = holidays[holidays < date]\n",
    "    return (date - past_holidays[-1]).days if past_holidays.size > 0 else None\n",
    "\n",
    "def get_month_section(date):\n",
    "    if date.day <= 10:\n",
    "        return \"Beginning\"\n",
    "    elif date.day <= 20:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"End\"\n",
    "\n",
    "\n",
    "\n",
    "# Add calculated columns to test and train data\n",
    "for df in [df_test, df_train]:\n",
    "    df['Weekday/Weekend'] = df['Date'].apply(get_weekday_or_weekend)\n",
    "    df['Days to Next Holiday'] = df['Date'].apply(lambda x: days_to_next_holiday(x, state_holidays))\n",
    "    df['Days After Last Holiday'] = df['Date'].apply(lambda x: days_after_last_holiday(x, state_holidays))\n",
    "    df['Month Section'] = df['Date'].apply(get_month_section)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Extra features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Weekday/Weekend</th>\n",
       "      <th>Days to Next Holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>Month Section</th>\n",
       "      <th>Day of Year</th>\n",
       "      <th>Week Number</th>\n",
       "      <th>Is Quarter Start</th>\n",
       "      <th>Is Quarter End</th>\n",
       "      <th>Is Month Start</th>\n",
       "      <th>Is Month End</th>\n",
       "      <th>Season</th>\n",
       "      <th>Promo x Weekend</th>\n",
       "      <th>Holiday x Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>260</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>260</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>260</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>260</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn - 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>260</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>Autumn - 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Store  DayOfWeek       Date  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       "0   1      1          4 2015-09-17   1.0      1            0              0   \n",
       "1   2      3          4 2015-09-17   1.0      1            0              0   \n",
       "2   3      7          4 2015-09-17   1.0      1            0              0   \n",
       "3   4      8          4 2015-09-17   1.0      1            0              0   \n",
       "4   5      9          4 2015-09-17   1.0      1            0              0   \n",
       "\n",
       "  Weekday/Weekend Days to Next Holiday  ...  Month Section Day of Year  \\\n",
       "0         Weekday                 None  ...            Mid         260   \n",
       "1         Weekday                 None  ...            Mid         260   \n",
       "2         Weekday                 None  ...            Mid         260   \n",
       "3         Weekday                 None  ...            Mid         260   \n",
       "4         Weekday                 None  ...            Mid         260   \n",
       "\n",
       "   Week Number  Is Quarter Start  Is Quarter End  Is Month Start  \\\n",
       "0           38             False           False           False   \n",
       "1           38             False           False           False   \n",
       "2           38             False           False           False   \n",
       "3           38             False           False           False   \n",
       "4           38             False           False           False   \n",
       "\n",
       "   Is Month End  Season Promo x Weekend  Holiday x Season  \n",
       "0         False  Autumn           False        Autumn - 0  \n",
       "1         False  Autumn           False        Autumn - 0  \n",
       "2         False  Autumn           False        Autumn - 0  \n",
       "3         False  Autumn           False        Autumn - 0  \n",
       "4         False  Autumn           False        Autumn - 0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Autumn\"  \n",
    "for df in [df_test, df_train]:\n",
    "  df['Day of Year'] = df['Date'].dt.dayofyear\n",
    "  df['Week Number'] = df['Date'].dt.isocalendar().week\n",
    "  df['Is Quarter Start'] = df['Date'].dt.is_quarter_start\n",
    "  df['Is Quarter End'] = df['Date'].dt.is_quarter_end\n",
    "  df['Is Month Start'] = df['Date'].dt.is_month_start\n",
    "  df['Is Month End'] = df['Date'].dt.is_month_end\n",
    "  df['Season'] = df['Date'].apply(get_season)\n",
    "\n",
    "  # Interaction and additional features\n",
    "  df['Promo x Weekend'] = (df['Weekday/Weekend'] == 'Weekend') & (df['Promo'] == 1)\n",
    "  df['Holiday x Season'] = df['Season'] + \" - \" + df['StateHoliday'].astype(str)\n",
    "\n",
    "# Save the updated datasets\n",
    "df_test.to_csv('test_updated.csv', index=False)\n",
    "df_train.to_csv('train_updated.csv', index=False)\n",
    "df_train.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling complete! Scaled datasets saved as 'train_scaled.csv' and 'test_scaled.csv'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Select numerical columns to scale (excluding non-numeric ones like 'Date' or categorical variables)\n",
    "numerical_columns_train = df_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_columns_test = df_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "#scaler_t = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both train and test data\n",
    "df_train[numerical_columns_train] = scaler.fit_transform(df_train[numerical_columns_train])\n",
    "df_test[numerical_columns_test] = scaler.fit_transform(df_test[numerical_columns_test])\n",
    "\n",
    "# Save the scaled datasets\n",
    "df_train.to_csv('train_scaled.csv', index=False)\n",
    "df_test.to_csv('test_scaled.csv', index=False)\n",
    "\n",
    "print(\"Scaling complete! Scaled datasets saved as 'train_scaled.csv' and 'test_scaled.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.40779878015432297\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "# Assuming you have already read `train_data`, `test_data`, and `store_data` into Pandas DataFrames\n",
    "df_train = pd.read_csv('train_scaled.csv')\n",
    "\n",
    "# Merge train_data and store_data\n",
    "train_data = df_train.merge(df_store, on='Store', how='left')\n",
    "\n",
    "# Sample a smaller subset of the data to speed up processing\n",
    "#train_data = train_data.sample(frac=0.9, random_state=42)  # Use 10% of the data\n",
    "\n",
    "# Feature engineering: Convert date column to datetime and extract useful features\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "train_data['Year'] = train_data['Date'].dt.year\n",
    "train_data['Month'] = train_data['Date'].dt.month\n",
    "train_data['Day'] = train_data['Date'].dt.day\n",
    "\n",
    "# Ensure consistent data types\n",
    "train_data['StateHoliday'] = train_data['StateHoliday'].astype(str)\n",
    "train_data['StoreType'] = train_data['StoreType'].astype(str)\n",
    "train_data['Assortment'] = train_data['Assortment'].astype(str)\n",
    "train_data['PromoInterval'] = train_data['PromoInterval'].astype(str)\n",
    "\n",
    "# Convert numeric columns with potential mixed types\n",
    "numerical_cols = ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear',\n",
    "                  'Promo2SinceWeek', 'Promo2SinceYear']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "\n",
    "# Select features and target\n",
    "features = [\n",
    "    'Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "    'Weekday/Weekend', 'Days to Next Holiday', 'Days After Last Holiday',\n",
    "    'Month Section', 'Day of Year', 'Week Number', 'Is Quarter Start',\n",
    "    'Is Quarter End', 'Is Month Start', 'Is Month End', 'Season',\n",
    "    'Promo x Weekend', 'Holiday x Season', 'StoreType', 'Assortment',\n",
    "    'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "    'Promo2SinceYear', 'PromoInterval'\n",
    "]\n",
    "\n",
    "target = 'Sales'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = train_data[features]\n",
    "y = train_data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing for numerical and categorical columns\n",
    "numerical_features = [\n",
    "    'Store', 'Promo', 'SchoolHoliday', 'Days to Next Holiday',\n",
    "    'Days After Last Holiday', 'Day of Year', 'Week Number',\n",
    "    'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "    'Promo2SinceYear'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'DayOfWeek', 'StateHoliday', 'Weekday/Weekend', 'Month Section',\n",
    "    'Is Quarter Start', 'Is Quarter End', 'Is Month Start', 'Is Month End',\n",
    "    'Season', 'Promo x Weekend', 'Holiday x Season', 'StoreType',\n",
    "    'Assortment', 'PromoInterval'\n",
    "]\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle NaN after coercion\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Create the pipeline with a Random Forest Regressor\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Loss function\n",
    "I chose Mean Absolute Error (MAE). This is a commonly used loss function for regression, which computes the absolute difference between predicted and actual values. It's easier to interpret and is less sensitive to outliers compared to Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Custom Loss Function: Mean Absolute Error (MAE)\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Post Prediction Analysis: Exploring Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Perform bootstrap and estimate confidence intervals\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[1;32m---> 36\u001b[0m lower_bound, upper_bound, bootstrap_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Ensure that the predictions and confidence intervals have the same shape\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_test) \u001b[38;5;241m==\u001b[39m bootstrap_preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in number of test samples between y_test and bootstrap_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[33], line 16\u001b[0m, in \u001b[0;36mbootstrap_predictions\u001b[1;34m(model_pipeline, X_train, y_train, X_test, n_iterations)\u001b[0m\n\u001b[0;32m     13\u001b[0m X_sample, y_sample \u001b[38;5;241m=\u001b[39m resample(X_train, y_train)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fit the model on the bootstrap sample\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[1;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    198\u001b[0m         X,\n\u001b[0;32m    199\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    203\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bethelhem.teka\\OneDrive - Catholic Relief Services\\Desktop\\KAIM04\\KIAM-Week04\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importance = model_pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Estimating Confidence Interval using Bootstrapping\n",
    "def bootstrap_predictions(model_pipeline, X_train, y_train, X_test, n_iterations=10):\n",
    "    \"\"\"\n",
    "    Generate bootstrap predictions and calculate confidence intervals.\n",
    "    \"\"\"\n",
    "    bootstrap_preds = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Bootstrap sampling: Randomly sample with replacement\n",
    "        X_sample, y_sample = resample(X_train, y_train)\n",
    "        \n",
    "        # Fit the model on the bootstrap sample\n",
    "        model_pipeline.fit(X_sample, y_sample)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = model_pipeline.predict(X_test)\n",
    "        \n",
    "        # Store the predictions\n",
    "        bootstrap_preds.append(y_pred)\n",
    "    \n",
    "    # Convert the list of predictions into a numpy array for easier analysis\n",
    "    bootstrap_preds = np.array(bootstrap_preds)\n",
    "    \n",
    "    # Calculate the confidence intervals (e.g., 95% CI)\n",
    "    lower_bound = np.percentile(bootstrap_preds, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_preds, 97.5, axis=0)\n",
    "    \n",
    "    return lower_bound, upper_bound, bootstrap_preds\n",
    "\n",
    "# Perform bootstrap and estimate confidence intervals\n",
    "from sklearn.utils import resample\n",
    "\n",
    "lower_bound, upper_bound, bootstrap_preds = bootstrap_predictions(model_pipeline, X_train, y_train, X_test)\n",
    "\n",
    "# Ensure that the predictions and confidence intervals have the same shape\n",
    "assert len(y_test) == bootstrap_preds.shape[1], \"Mismatch in number of test samples between y_test and bootstrap_preds\"\n",
    "\n",
    "# Visualize the predictions and confidence intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values, label=\"True Values\", color=\"black\", alpha=0.5)\n",
    "plt.plot(bootstrap_preds.mean(axis=0), label=\"Predictions (mean)\", color=\"blue\")\n",
    "plt.fill_between(np.arange(len(y_test)), lower_bound, upper_bound, color='gray', alpha=0.3, label=\"95% Confidence Interval\")\n",
    "plt.xlabel(\"Test Sample Index\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(\"Predictions with 95% Confidence Interval\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Serealize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 14-01-2025-14-47-55-548244.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Assuming you already have your model trained, like 'model'\n",
    "\n",
    "# 1. Get the current timestamp in the format \"dd-mm-yyyy-HH-MM-SS-ff\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S-%f\")\n",
    "\n",
    "# 2. Define the model filename using the timestamp\n",
    "model_filename = f\"{timestamp}.pkl\"\n",
    "\n",
    "# 3. Save the model using joblib\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Building model with deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.4912\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4395\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2733\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0826\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0329\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0268\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0272\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0226\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0175\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0195\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step\n",
      "Prediction for the next value: -0.023620009422302246\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Assuming you have data in `data`, for example, stock prices, etc.\n",
    "# Let's generate some dummy time series data for demonstration purposes\n",
    "# In practice, replace this with your own dataset\n",
    "data = np.sin(np.linspace(0, 100, 200))  # Just an example: a sine wave dataset\n",
    "\n",
    "# Reshape the data for time series forecasting\n",
    "data = data.reshape((len(data), 1))\n",
    "\n",
    "# Use a TimeSeriesGenerator to create sequences of data for training\n",
    "sequence_length = 10  # Number of time steps in each sequence\n",
    "batch_size = 16\n",
    "\n",
    "generator = TimeseriesGenerator(data, data, length=sequence_length, batch_size=batch_size)\n",
    "\n",
    "# Define the LSTM model with two layers\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with 50 units\n",
    "model.add(LSTM(50, activation='relu', input_shape=(sequence_length, 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Adding dropout for regularization\n",
    "\n",
    "# Second LSTM layer with 50 units\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dropout(0.2))  # Adding dropout for regularization\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))  # Predicting one value (can be modified for multi-output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(generator, epochs=10, verbose=1)\n",
    "\n",
    "# After training, you can make predictions using the model\n",
    "# Here we use the last part of the input data (in this case, the last 10 values)\n",
    "test_input = data[-sequence_length:].reshape((1, sequence_length, 1))\n",
    "\n",
    "# Predict the next value (next time step)\n",
    "prediction = model.predict(test_input)\n",
    "print(f'Prediction for the next value: {prediction[0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = model_pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Estimating Confidence Interval using Bootstrapping\n",
    "def bootstrap_predictions(model_pipeline, X_train, y_train, X_test, n_iterations=10):\n",
    "    \"\"\"\n",
    "    Generate bootstrap predictions and calculate confidence intervals.\n",
    "    \"\"\"\n",
    "    bootstrap_preds = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Bootstrap sampling: Randomly sample with replacement\n",
    "        X_sample, y_sample = resample(X_train, y_train)\n",
    "        \n",
    "        # Fit the model on the bootstrap sample\n",
    "        model_pipeline.fit(X_sample, y_sample)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = model_pipeline.predict(X_test)\n",
    "        \n",
    "        # Store the predictions\n",
    "        bootstrap_preds.append(y_pred)\n",
    "    \n",
    "    # Convert the list of predictions into a numpy array for easier analysis\n",
    "    bootstrap_preds = np.array(bootstrap_preds)\n",
    "    \n",
    "    # Calculate the confidence intervals (e.g., 95% CI)\n",
    "    lower_bound = np.percentile(bootstrap_preds, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_preds, 97.5, axis=0)\n",
    "    \n",
    "    return lower_bound, upper_bound, bootstrap_preds\n",
    "\n",
    "# Perform bootstrap and estimate confidence intervals\n",
    "from sklearn.utils import resample\n",
    "\n",
    "lower_bound, upper_bound, bootstrap_preds = bootstrap_predictions(model_pipeline, X_train, y_train, X_test)\n",
    "\n",
    "# Ensure that the predictions and confidence intervals have the same shape\n",
    "assert len(y_test) == bootstrap_preds.shape[1], \"Mismatch in number of test samples between y_test and bootstrap_preds\"\n",
    "\n",
    "# Visualize the predictions and confidence intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values, label=\"True Values\", color=\"black\", alpha=0.5)\n",
    "plt.plot(bootstrap_preds.mean(axis=0), label=\"Predictions (mean)\", color=\"blue\")\n",
    "plt.fill_between(np.arange(len(y_test)), lower_bound, upper_bound, color='gray', alpha=0.3, label=\"95% Confidence Interval\")\n",
    "plt.xlabel(\"Test Sample Index\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(\"Predictions with 95% Confidence Interval\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Model Serving API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
